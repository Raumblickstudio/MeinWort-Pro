import OpenAI from 'openai';
import { AudioRecording } from './audioService';

export interface TranscriptionOptions {
  language?: string;
  temperature?: number;
  response_format?: 'json' | 'text' | 'srt' | 'verbose_json' | 'vtt';
  prompt?: string;
}

export interface TranscriptionResult {
  text: string;
  language?: string;
  duration?: number;
  confidence?: number;
}

export class TranscriptionService {
  private client: OpenAI;
  private isInitialized: boolean = false;

  constructor() {
    // Client wird lazy initialisiert um API Key Validierung zu erm√∂glichen
    this.client = this.initializeClient();
  }

  private initializeClient(): OpenAI {
    const apiKey = import.meta.env.VITE_OPENAI_API_KEY;
    
    if (!apiKey || apiKey === 'your-openai-api-key-here') {
      throw new Error(
        'OpenAI API Key nicht gefunden. ' +
        'Bitte setzen Sie VITE_OPENAI_API_KEY in der .env Datei.'
      );
    }

    if (!apiKey.startsWith('sk-')) {
      throw new Error('Ung√ºltiger OpenAI API Key Format. Key sollte mit "sk-" beginnen.');
    }

    const client = new OpenAI({
      apiKey,
      organization: import.meta.env.VITE_OPENAI_ORGANIZATION,
      baseURL: import.meta.env.VITE_OPENAI_BASE_URL,
      dangerouslyAllowBrowser: true, // Erforderlich f√ºr Browser-Umgebung
    });

    this.isInitialized = true;
    return client;
  }

  async validateApiKey(): Promise<boolean> {
    try {
      if (!this.isInitialized) {
        this.client = this.initializeClient();
      }

      // Teste API-Verbindung mit einem kleinen Request
      const models = await this.client.models.list();
      const hasWhisperModel = models.data.some(model => 
        model.id.includes('whisper')
      );

      if (!hasWhisperModel) {
        console.warn('‚ö†Ô∏è Keine Whisper-Modelle gefunden, aber API-Key ist g√ºltig');
      }

      console.log('‚úÖ OpenAI API Key validiert');
      return true;

    } catch (error: any) {
      console.error('‚ùå API Key Validierung fehlgeschlagen:', error);
      
      if (error?.status === 401) {
        throw new Error('OpenAI API Key ung√ºltig oder abgelaufen.');
      } else if (error?.status === 429) {
        throw new Error('OpenAI API Rate Limit erreicht. Bitte versuchen Sie es sp√§ter erneut.');
      } else if (error?.status === 403) {
        throw new Error('OpenAI API Zugriff verweigert. √úberpr√ºfen Sie Ihre Berechtigung f√ºr Whisper.');
      }
      
      throw new Error(`API Verbindungsfehler: ${error?.message || 'Unbekannter Fehler'}`);
    }
  }

  async transcribeAudio(
    recording: AudioRecording,
    options: TranscriptionOptions = {}
  ): Promise<TranscriptionResult> {
    try {
      if (!this.isInitialized) {
        this.client = this.initializeClient();
      }

      // Validierungen
      if (!recording.blob || recording.blob.size === 0) {
        throw new Error('Keine g√ºltige Audio-Datei vorhanden');
      }

      // OpenAI hat ein 25MB Limit
      const maxSize = 25 * 1024 * 1024; // 25MB
      if (recording.blob.size > maxSize) {
        throw new Error(`Audio-Datei zu gro√ü (${Math.round(recording.blob.size / 1024 / 1024)}MB). Maximum: 25MB`);
      }

      // üîá SILENCE DETECTION: Sehr kurze Aufnahmen als Stille filtern
      if (recording.duration < 300) {
        console.warn('‚ö†Ô∏è Audio-Aufnahme sehr kurz (wahrscheinlich Stille):', recording.duration, 'ms');
        return {
          text: '[Aufnahme zu kurz - bitte sprechen Sie l√§nger]',
          duration: recording.duration,
          confidence: 0
        };
      }

      // üîá ADVANCED SILENCE DETECTION: Audio-Daten auf Stille pr√ºfen
      const silenceThreshold = await this.detectSilence(recording.blob);
      if (silenceThreshold > 0.8) {
        console.warn('üîá Audio-Aufnahme haupts√§chlich Stille erkannt:', silenceThreshold * 100, '% Stille');
        return {
          text: '[Nur Stille erkannt - bitte lauter sprechen]',
          duration: recording.duration,
          confidence: 0
        };
      }
      
      // Warnung bei sehr kurzen Aufnahmen
      if (recording.duration < 500) {
        console.warn('‚ö†Ô∏è Audio-Aufnahme m√∂glicherweise zu kurz f√ºr zuverl√§ssige Erkennung:', recording.duration, 'ms');
      }

      console.log(`üîÑ Starte Transkription: ${Math.round(recording.blob.size / 1024)}KB, ${recording.duration}ms`);
      console.log(`üéß Audio-Details: Type=${recording.blob.type}, Size=${recording.blob.size} bytes`);

      // ‚ö° SPEED OPTIMIZATION: Direkter Blob-Upload ohne File-Wrapper f√ºr bessere Performance
      const audioBlob = recording.blob;

      // Standard-Optionen
      const transcriptionOptions = {
        model: 'whisper-1',
        language: options.language || 'de', // Deutsch als Standard
        response_format: options.response_format || 'verbose_json' as const,
        temperature: options.temperature ?? 1.0, // üîä MAXIMUM EMPFINDLICHKEIT: H√∂chste Temperatur f√ºr aggressivste leise Sprache-Erkennung
        prompt: options.prompt || 'Dies ist deutsche Sprache. Bitte transkribiere ALLE Audio-Signale, auch extrem leise Fl√ºstern oder undeutliche Laute. Nutze maximale Empfindlichkeit.',
        ...options,
      };

      const startTime = Date.now();
      console.log(`üåê Sende an Whisper API mit Optionen:`, {
        model: transcriptionOptions.model,
        language: transcriptionOptions.language,
        temperature: transcriptionOptions.temperature,
        response_format: transcriptionOptions.response_format
      });

      // üîÑ SMART RETRY: Intelligente Wiederholung bei schlechter Qualit√§t
      const response: any = await this.transcribeWithRetry(audioBlob, transcriptionOptions, 2);

      const processingTime = Date.now() - startTime;
      console.log(`‚ö° Whisper API Response erhalten in ${processingTime}ms`);

      // Response verarbeiten
      let result: TranscriptionResult;

      if (typeof response === 'string') {
        // Einfacher Text-Response
        result = {
          text: response.trim(),
          duration: recording.duration,
        };
      } else if (response && typeof response === 'object' && 'text' in response) {
        // Verbose JSON Response
        result = {
          text: response.text.trim(),
          language: response.language,
          duration: recording.duration,
        };

        // Zus√§tzliche Metadaten aus verbose_json falls verf√ºgbar
        if ('segments' in response && Array.isArray(response.segments)) {
          const avgConfidence = response.segments.reduce((sum: number, segment: any) => 
            sum + (segment.avg_logprob || 0), 0
          ) / response.segments.length;
          
          // Log-Wahrscheinlichkeit zu Confidence Score konvertieren (ungef√§hr)
          result.confidence = Math.max(0, Math.min(1, Math.exp(avgConfidence)));
        }
      } else {
        throw new Error('Unerwartetes Response-Format von OpenAI API');
      }

      // Validierung des Ergebnisses - Nur bei wirklich leerem Text Fallback
      if (!result.text) {
        console.warn('‚ö†Ô∏è Whisper hat null/undefined Text zur√ºckgegeben');
        result.text = '[Keine Sprache erkannt - bitte sprechen Sie lauter oder l√§nger]';
        result.confidence = 0;
      } else if (result.text.trim().length === 0) {
        console.warn('‚ö†Ô∏è Whisper hat nur Leerraum zur√ºckgegeben');
        result.text = '[Keine Sprache erkannt - bitte sprechen Sie lauter oder l√§nger]';
        result.confidence = 0;
      } else {
        // Erfolgreiche Transkription - keine √Ñnderung n√∂tig
        console.log('‚úÖ Whisper hat Text erkannt:', result.text.substring(0, 50) + '...');
      }

      console.log(`‚úÖ Transkription erfolgreich: ${result.text.length} Zeichen in ${processingTime}ms`);
      console.log(`üìù Ergebnis: "${result.text.substring(0, 100)}${result.text.length > 100 ? '...' : ''}"`);

      return result;

    } catch (error: any) {
      console.error('‚ùå Transkription fehlgeschlagen:', error);

      // OpenAI spezifische Fehler
      if (error?.status === 400) {
        throw new Error('Ung√ºltiges Audio-Format oder -inhalt');
      } else if (error?.status === 413) {
        throw new Error('Audio-Datei zu gro√ü (Maximum: 25MB)');
      } else if (error?.status === 429) {
        throw new Error('Zu viele Anfragen. Bitte warten Sie einen Moment.');
      } else if (error?.status === 500) {
        throw new Error('OpenAI Server-Fehler. Bitte versuchen Sie es sp√§ter erneut.');
      }

      // Netzwerk-Fehler
      if (error?.code === 'NETWORK_ERROR' || !navigator.onLine) {
        throw new Error('Keine Internetverbindung verf√ºgbar');
      }

      throw new Error(
        `Transkription fehlgeschlagen: ${error?.message || 'Unbekannter Fehler'}`
      );
    }
  }

  // Utility: Unterst√ºtzte Audio-Formate pr√ºfen
  static getSupportedFormats(): string[] {
    return [
      'audio/webm',
      'audio/wav',
      'audio/mp4',
      'audio/mpeg',
      'audio/ogg',
      'audio/flac',
    ];
  }

  // Utility: Gesch√§tzte Kosten berechnen (grobe Sch√§tzung)
  static estimateCost(durationMs: number): number {
    const minutes = durationMs / 60000;
    return Math.max(0.006, minutes * 0.006); // $0.006 per Minute
  }

  // Cleanup f√ºr Browser-spezifische Ressourcen
  // üîÑ SMART RETRY: Intelligente Wiederholung mit verschiedenen Parametern
  private async transcribeWithRetry(
    audioBlob: Blob, 
    baseOptions: any, 
    maxRetries: number = 2
  ): Promise<any> {
    const retryConfigs = [
      // Versuch 1: Standard-Parameter
      { ...baseOptions },
      // Versuch 2: H√∂here Temperatur f√ºr schwierige Audio
      { ...baseOptions, temperature: Math.min(1.0, baseOptions.temperature + 0.3) },
      // Versuch 3: Anderer Prompt f√ºr sehr schwierige F√§lle
      { ...baseOptions, 
        temperature: 1.0, 
        prompt: 'Dies ist deutsche Sprache mit m√∂glicherweise schlechter Audioqualit√§t. Nutze maximale Empfindlichkeit und aggressive Erkennung.' 
      }
    ];

    let lastError: any;
    
    for (let attempt = 0; attempt <= maxRetries; attempt++) {
      try {
        const config = retryConfigs[attempt] || retryConfigs[retryConfigs.length - 1];
        console.log(`üîÑ Whisper Versuch ${attempt + 1}/${maxRetries + 1} mit Temperatur ${config.temperature}`);
        
        const response = await this.client.audio.transcriptions.create({
          file: new File([audioBlob], 'recording.webm', { type: audioBlob.type }),
          ...config,
        });

        // Erfolg validieren
        if (response && (typeof response === 'string' || (response.text && response.text.trim()))) {
          if (attempt > 0) {
            console.log(`‚úÖ Whisper erfolgreich nach ${attempt + 1} Versuchen`);
          }
          return response;
        } else {
          throw new Error('Leere oder ung√ºltige Antwort');
        }
        
      } catch (error: any) {
        lastError = error;
        console.warn(`‚ö†Ô∏è Whisper Versuch ${attempt + 1} fehlgeschlagen:`, error?.message);
        
        // Bei letztem Versuch: Fehler weiterwerfen
        if (attempt === maxRetries) {
          console.error(`‚ùå Alle ${maxRetries + 1} Whisper-Versuche fehlgeschlagen`);
          throw lastError;
        }
        
        // Kurze Pause vor n√§chstem Versuch
        await new Promise(resolve => setTimeout(resolve, 500 * (attempt + 1)));
      }
    }
    
    throw lastError;
  }

  // üîá SILENCE DETECTION: Audio-Blob auf Stille analysieren
  private async detectSilence(blob: Blob): Promise<number> {
    try {
      const arrayBuffer = await blob.arrayBuffer();
      const audioContext = new AudioContext();
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      
      const channelData = audioBuffer.getChannelData(0);
      const sampleRate = audioBuffer.sampleRate;
      const frameSize = Math.floor(sampleRate * 0.1); // 100ms Frames
      
      let silentFrames = 0;
      let totalFrames = 0;
      
      for (let i = 0; i < channelData.length; i += frameSize) {
        const frame = channelData.slice(i, i + frameSize);
        const rms = Math.sqrt(frame.reduce((sum, sample) => sum + sample * sample, 0) / frame.length);
        
        // Stille-Schwellwert: sehr niedrige RMS-Werte
        if (rms < 0.01) {
          silentFrames++;
        }
        totalFrames++;
      }
      
      const silenceRatio = totalFrames > 0 ? silentFrames / totalFrames : 1;
      console.log(`üîá Silence Detection: ${(silenceRatio * 100).toFixed(1)}% stille Frames`);
      
      audioContext.close();
      return silenceRatio;
      
    } catch (error) {
      console.warn('üîá Silence Detection fehlgeschlagen (nicht kritisch):', error);
      return 0; // Bei Fehler annehmen dass Audio vorhanden ist
    }
  }

  cleanup(): void {
    // Momentan keine spezielle Cleanup-Logik erforderlich
    this.isInitialized = false;
  }
}

// Singleton Instance
export const transcriptionService = new TranscriptionService();